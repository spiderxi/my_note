# 1. 深度学习

## 1. 神经网络简介

_常见的激活函数有哪些?_

```
* sigmoid函数
y(x) = 1/1+e^(-x)

* relu函数
y(x) = max(0, x)
```

_输出层的激活函数有哪些?_

```
* 恒等函数(回归问题)

* softmax函数(分类问题)
```

_为什么激活函数不能使用线性函数?_

```
当激活函数为线性函数时, 多层神经网络一定可以使用单层神经网络表示, 多层神经网络没有意义
```

_神经网络前向传播的过程?_

```
1. 将输入信号X正则化为0~1之间的值

2. 重复计算A = f(W.X + B)

3. 计算输出层Y = f(A)
```

## 2. SGD 算法

_常见的损失函数有哪些?_

```
* 均方误差(回归问题)

* 交叉熵(聚类问题)
```

_为什么一般不使把识别错误率当作损失函数?_

```
当微调参数时, 错误率往往没有变化(梯度为0), 因为错误率是离散的
```

_学习率过大和过小会造成什么结果, 学习率如何设定?_

```
* 过大: Loss函数值在最小值附近震荡, 始终不能收敛到最小值

* 过小: 收敛需要的迭代次数增多, 并且更容易收敛到局部最小值

实际应用中, 学习率应当采用衰减算法或模拟退火算法动态调整
```

_讲一下 SGD(随机梯度下降法)步骤?_

```
1. 将训练集随机分为多个互斥的mini-batch

2. 遍历mini-batch, 在每个mini-batch上计算Loss函数的梯度并进行梯度下降

3. 重复1 2

tip: 进行一次1 2步骤称为一个epoch
```

## 3. BP 算法

_有哪些方式可以计算 Loss 函数的梯度?_

```
* 微分法: 梯度 = [Loss(W + h) - Loss(W-h)] / 2h, h=1e-4

* BP算法

tip: 微分法速度很慢, 因为即使最简单的神经网络中参数也有百万个, 每次计算一次梯度要进行百万次矩阵运算
```
