# 神经网络基础

## 感知机与神经网络
_感知机为什么无法解决非线性问题？_
```
感知机是只有N个输入神经元和1个输出神经元的模型， 等价于函数y = sign(w.x + b)
🌟 其中wx+b = 0表示N维空间中的N-1维超平面， 例如二维空间中表示一条线
🌟 sign函数是一个符号判别函数， 在这个超平面之上的被分为正类，在平面之下的为负类
🌟 本质上1个输出神经元代表一个超平面， 非线性可分问题不能靠1个超平面区分开

🌙 例如AND/NOT/OR运算都是线性运算可以用感知机实现， XOR运算不是线性运算无法用感知机实现
```

_常见的隐藏层激活函数有哪些?_

```
🌟sigmoid： y(x) = 1/1+e^(-x)
🌟reLU函数：y(x) = max(0, x)
```

_为什么MLP只需要引入一个隐含层后可以拟合出任意连续函数？_
```
🌟 每个隐藏神经元通过wx+b定义一个超平面（特征平面），并通过非线性激活函数（如ReLU）带强度地选择性地响应某一侧的半空间
🌟 输出层对所有隐藏神经元进行带权重的线性组合，可以构造出复杂的决策区域（如多边形）
🌟 只要隐藏神经元数量足够多，且激活函数非线性（如ReLU），该网络就能逼近任意连续函数

🌙 ReLU函数返回0代表没有这个特征（不在特征的半空间内/不在特征平面上方）， 返回0.5代表在特征平面上方（基本符合某特征）， 返回值越大代表此特征平面越有效
🌙 激活函数返回值 = 半空间选择强度 = 特征强度
```

_为什么隐藏层的激活函数必须是非线性函数？_
```
三层神经网络等效于函数f(X) = h(h(W.h(WX+B)+B)+B), 其中h是每一层的激活函数， 假设h为线性函数， 由于WX+B也是线性函数：
多个线性函数的复合f(X)也是线性函数无法解决非线性问题
```

_现代神经网络为什么不用sign函数作为激活函数？_
```
🌟 现代神经网络使用梯度下降法， sign无法求导

🌙 sign作为特征平面判断时， 只有符合特征和不符合特征两种情况（返回-1/1）
```

## 基础神经网络
_输出层的激活函数有哪些?_
```
🌟恒等函数(回归问题)： h(x) = x
🌟softmax函数(分类问题)

🌙 输出层的激活函数可以是任意函数， 但从梯度训练的角度来说最好能求导
```

_神经网络前向传播的过程?_
```
🌟 将输入信号X正则化为0~1之间的值
🌟 重复计算隐藏层神经元值A = 隐藏层激活函数(W.X + B)，并记录中间变量便于后向传播的计算
🌟 计算输出层Y = 输出层激活函数(A)
```

_为什么增加神经网络深度比增加宽度效果更好？_
```
🌟 近似，神经网络收敛计算次数随宽度平方增加， 随深度线性增加

🌙复杂问题往往可以拆分为多个简单问题（甚至模块化思考，复用模块）， 增加宽度相当于增加每一步的思考量减少思考步骤， 增加深度相当于增加思考步骤减少每一步思考量， 最终都能实现结果
```

## SGD算法

_怎样计算出神经网络中W和B等参数呢？_
```
通过已知的X和Y（训练集）， 让神经网络函数f去拟合X和Y使得f(X)逼近Y, 定义损失函数Loss(W, B) = Y-f(X), 找到使得Loss最小的点(W,B)即可， 从梯度的视角来说：
1️⃣ 从任意初始点（W0, B0）开始， 求当前点的梯度，朝着梯度下降的方向前进一段距离理论能最快接近局部最低点
2️⃣ 假设前进一段距离后点为（W1, B1）， 重复上述过程， 直到找到一个局部最低点

🌙 神经网络找到的局部最低点不一定是全局最低点
🌙 每次朝梯度下降方向前进的距离被称为“学习率”
```

_常见的损失函数有哪些?_
```
🌟 均方误差函数(回归问题)
🌟 交叉熵函数(分类问题, 和softmax函数一起使用时可以简化BP计算)
```

_讲一下 SGD(随机梯度下降法)步骤?_
```
1️⃣将训练集随机分为多个互斥的mini-batch
2️⃣遍历mini-batch, 在每个mini-batch上计算Loss函数的梯度并进行梯度下降
3️⃣重复1️⃣2️⃣

🌙进行一次1️⃣2️⃣步骤称为一个epoch
🌙mini-batch的大小等参数被称为超参数（和模型参数W/B区分开， 训练过程中的参数）
```

## BP算法

_有哪些方式可以计算 Loss 函数的梯度?_

```
🌟微分法: 梯度 = [Loss(W + h) - Loss(W-h)] / 2h, h=微分=1e-4
🌟BP算法

🌙 微分法速度很慢, 因为即使最简单的神经网络中参数也有百万个, 每次计算一次梯度要进行百万次矩阵运算
🌙 正是有了BP算法， 神经网络才得以发展， BP算法灵感也来源与人类的神经元反馈过程
```

_反向传播算法计算偏导的原理是什么?_
```
链式法则

🌙 神经网络本质上是多个函数的符合, 反向传播相当于对函数Loss(Y(h(X)))从外到内依次求导, 并将导数从后往前传播
```

_一个实际的神经网络分为哪些层?_

```
🌟 Affine层： WX+B计算
🌟 sigmoid层/reLU层： 激活函数计算
🌟 softmax-with-loss层/mean-square-with-loss层：输出层

🌙 上面提到的是最简单的神经网络， 实际上有很多其他层
```

## 神经网络优化技巧

_学习率过大和过小会造成什么结果? 学习率如何设定?_

```
🌟 过大: Loss函数值在最小值附近震荡, 始终不能收敛到最小值
🌟 过小: 收敛需要的迭代次数增多, 并且更容易收敛到局部最小值

🌙 Adam算法中学习率会动态衰减， Adam算法还模拟了梯度下降时的物理惯性，避免过快掉入局部最低点陷阱
```

_初始权重参数如何设置?_

```
He初始化： 随机正态分布数~N(0, sqrt(2/in)), 适合reLU函数
```

_什么是梯度消失和梯度爆炸?_
```
🌟 梯度消失: 激活函数为S型曲线时, 连续多个神经网络层梯度小于1, 相乘导致梯度向量无穷小

🌟 梯度爆炸: 激活函数为S型曲线时, 连续多个神经网络层梯度很大, 相乘导致梯度溢出
```

_Batch Normalization层做了什么?_

```
将输入信号转换为（0，1）之间的信号
```

_有哪些方式可以防止神经网络过拟合?_
```
Dropout方法： 添加DropOut Layer随机删除神经网络中的神经元
```

_讲一下Data Augmentation方法?_

```
通过对训练集的样本图像进行平移, 旋转等技巧变换出新的训练集, 从而增大训练集大小
```

# 2. 特殊网络

## CNN

_CNN 相较于传统全连接神经网络的优点?_

```
CNN网络的前几层使用了Convolution Layer, Conv Layer可以提取二维图形中的局部特征(边缘, 形状, 纹理, 物体)

tip: Conv Layer相较于Affine Layer的优点是将输入的图像当作二维而不是一维, 二维图像进行卷积运算可以提取局部特征
```

![1719229475199](image/draft/1719229475199.png)

_CNN 中有哪些独特的 Layer?_

```
* Convolution Layer

* Max Pooling Layer
```

_什么是卷积运算?_

```
通过一个称为"卷积核"或"滤波器"的小矩阵与一个更大的矩阵进行元素对元素的加权求和，来提取或增强某些特征
```

![1719231136266](image/draft/1719231136266.png)

## RNN

## ResNet

## GAN

## LSTM

## U-Net

## 深度学习网络

_为什么更深层的网络学习效果往往更好?_

```
直观上讲, 深层网络每一层可以单独学习一方面的特征, 而浅层网络需要一次性识别多方面的特征
```

_为什么过度加深层会很多情况下学习将不能顺利进行?_

```
随着网络深度的增加, 参数增多，损失函数的曲面可能变得更加复杂，存在更多的局部最小值、鞍点或平坦区域
```

# 参考书籍
🌟 深入学习入门-斋藤康毅