# 1. 向量

## 1. 范数和内积

**_向量的范数(norm)和内积(inner production)的实际意义?_**

```
>> 范数表示向量的长短

>> 内积表示两个向量的对齐程度
```

**_向量的 L2 范数和内积如何计算?_**

$$
内积 = <\vec{u}, \vec{v}> = u_1v_1 + u_2v_2 + u_3v_3 = |\vec{U}|.|\vec{V}|.cos(\theta)
$$

$$
L2范数 = |\vec{u}|_{L2} = \sqrt{<\vec{u}, \vec{u}>} = \sqrt{u_1^2 + u_2^2 + u_3^2}
$$

**_函数和向量有什么关系?_**

```
>> 一个函数可以看作是一个无穷维度的向量(每一个x可以看作一个维度, 该维度的值为f(x))
```

$$
函数||f||的范数 = \sqrt{\int f^2 dx}
$$

$$
函数f和函数g的内积<<f, g>> = \int (f.g)  dx\\
$$

## 2. 线性映射

**_线性变换的代数含义?_**

![1698390429277](image/linear-algebra/1698390429277.png)

**_线性映射(linear map)和仿射(affine map)的区别?_**

```
仿射变换本质上是先对进行线性变换, 再进行平移
```

**_讲一下 Schmidt 正交化算法?_**

```
>> 算法目的: 将一个非正交基向量集合转为正交单位基向量集合

>> 算法思想: 递归算法, 向量A减去A在正交集合的每个向量方向上的分量后, 再单位化, 最后加入正交集合

>> 为什么要正交化? 任意一个向量 === 该向量在各个正交基向量上的投影分量之和
```

## 3. 叉积

**_叉积在三维空间中的含义?_**

```
两个向量的叉积:
* 方向 = 垂直于这两个向量组成的平面, 遵循右手法则

* 长度 = 两个向量围成的四边形的面积
```

**_如何快速记忆一个三维向量的叉积公式?_**

```
使用行列式可以快速记忆叉积计算公式
```

![1699516350763](image/linear-algebra/1699516350763.png)

**_叉积在二维空间中的含义?_**

```
>> 二维向量的叉积是一个标量, 大小为两个向量围成图形的面积

>> (x1, y1) × (x2, y2) = x1y2 - y1x2
```

**_如何求两个向量的夹角?_**

```
>> cos(夹角) =  向量内积/向量范数的乘积

>> sin(夹角) = 叉积的范数 / 向量范数的乘积
```

## 4. 插值

**_两个向量的插值向量有什么用?_**

```
向量a和向量b的插值向量是一个关于时间的函数: c(t), 满足:
>> c(0) = a

>> c(1) = b

c(t)可以作为向量a到向量b的过渡向量
```

**_向量的线性插值和角度线性插值公式?_**

$$
线性插值: \vec{c}(t) = (1-t)\vec{a} + t\vec{b}\\
角度线性插值: \vec{c}(t) = \frac{\sin[(1-t)\theta]}{\sin\theta}\vec{a} + \frac{\sin[t\theta]}{\sin\theta}\vec{b}
$$

## 5. 虚数和向量

**_为什么会有虚数单位 i?_**

```
每个一个复数对应一个二维向量, 一个复数乘i 等价于 对应的二维向量逆时针转90度
```

**_复数的乘积的几何含义是什么?_**

$$
\begin{align*}
&每一个复数等价于一个线性变换, 一个复数w乘以复数z代表将w对应的线性变换施加到z代表的向量上

\\&>> 旋转度数 由w的辐角主值决定

\\&>> 伸缩倍数由 w的模长决定
\end{align*}
$$

**_复数作为 e 的指数的意义是什么?_**

$$
e^z 等价于函数Exp(z) = 1 + z + \frac{z^2}{2}  + \frac{z^3}{6} + ...  + \frac{z^n}{n!} + ...
$$

**_如何证明 Euler 公式?_**

$$
\\Euler \qquad Equation: e^{ix} = \cos(x) + i\sin(x)\\
将e^{ix}进行泰勒展开, 然后将展开的多项式各项然后为\cos(x), \sin(x)的泰勒展开式的项
$$

# 2. 矩阵

## 1. 矩阵和向量

**_一个矩阵可以表示哪些实际含义?_**

```
>> 一个矩阵可以表示一个向量线性变换

>> 一个矩阵可以表示一个线性方程组(Ax = b)

>> 一个对称矩阵可以表示一个二次型(xAx^T)
```

## 2. 矩阵代数

**_如何求解一个矩阵向量方程式 Ax = b?_**

```
1. 写出增广矩阵

2. 进行初等行变换, 变成阶梯型矩阵

3. 确定自由变量, 反代求出剩余变量
```

**_初等行变换有哪三种, 行变换的代数含义是什么?_**

```
初等行变换包括:

>> 交换两行
>> 单独一行乘一个常数
>> A行加上B行的倍数

初等行变换等价于: 右乘 I进行相同行变换后的矩阵I'
```

**_Ax = 0 和 Ax = b 的解集的关系是什么?_**

Ax=0

$$
齐次线性方程的解为\vec{x}_{齐次} = a\vec{u} + b\vec{v},
\\其中a,b为自由变量, \vec{u}和\vec{v}为解空间基向量
$$

Ax=b

$$
齐次线性方程的解为\vec{x}_{非齐次} = \vec{p} + \vec{x}_{齐次}, \\
\vec{p}为一个Ax = b的一个特解
$$

**_Ax = 0 有非零解的条件?_**

```
有非零解说明经过A的线性变换后所有向量围成的体积均为零, 所以A的行列式为零
```

**_逆矩阵的几何含义是什么, 如何求逆矩阵?_**

```
>> 几何含义: 原矩阵对应的线性变换的逆变换

>> 对增广矩阵进行初等行变换可以求逆矩阵: (A, E) => (E, A^(-1))
```

**_二维矩阵的逆矩阵如何求?_**

$$
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}^{-1} = (ad-cb)
\begin{bmatrix}
d & -b\\
-c & a
\end{bmatrix}
\\
口诀: 主交换, 反取逆, 除行列式
$$

## 3. 矩阵运算

**_为什么使用矩阵分块的方法计算矩阵乘法?_**

```
>> 对于稀疏矩阵, 分块后有大量的0矩阵, 可以加快乘法

>> 更好地利用cache
```

**_求解线性方程 Ax=b 时为什么要对系数矩阵进行 LU 分解?_**

```
进行LU分解后, 当b变换时就可以快速地计算出对应的解x

原理: 分解后原方程变为两个方程: Ux = y; Ly = b, 由于L和U为阶梯矩阵可以快速计算出y, 然后计算出x
```

## 4. 行列式

**_矩阵的行列式的几何含义是什么?_**

```
A的行列式 === A的列向量围成的体积 === 经过A对应的线性变换后向量围成的体积的变化倍率
```

**_二维/三维矩阵行列式公式是什么?_**

$$
\begin{vmatrix}
a & b\\
c & d
\end{vmatrix} = ad-bc
\\
\begin{vmatrix}
a & b & c\\
d & e & f\\
g & h & i
\end{vmatrix} = (aei+bfg+dhc) - (ceg+ahf+bdi)
$$

**_行列式在线性方程组求解中的作用?_**

$$
\begin{align*}
对于线性方程组 A\vec{x} = \vec{b}, &det(A)可以用于:\\
&1. 判断方程组是否有唯一解(det(A)=0时有唯一解)\\

&2. 根据Cramer's Rule快速得出唯一解
\end{align*}
\\
Cramer's Rule: 唯一解\vec{x_0} = \frac{\vec{D}}{det(A)}
$$

## 5. 矩阵的特征

**_什么是矩阵的秩, 有什么用?_**

```
>> 定义: 矩阵进行行变换后得到的阶梯型矩阵的非零行的个数

>> rank(A) 可以用于判断 方程组Ax = b的非自由变量的个数
```

**_一个矩阵的特征向量和特征值的几何含义?_**

```
>> 几何定义: 特征向量在经过矩阵A对应的线性变换后向量的方向不改变(Ax = kx)

>> 特征向量就是一个向量在经过矩阵对应的线性变换后趋向的方向

>> 特征值就是趋向这个方向的速度

tips: 一个向量a经过无数次矩阵对应的变换后, 无限接近特征值最大的特征向量的方向
```

**_如何求一个矩阵的特征值?_**

$$
\begin{align*}
&A\vec{x} = \lambda \vec{x}\\
\Rightarrow &(A-\lambda E)\vec{x} = \vec{0}\\
\Rightarrow &f(\lambda) = |A-\lambda E| = 0\\
\end{align*}
\\所以特征方程f(\lambda) = |A-\lambda E| = 0的根即为特征值
$$

**_如何加快计算矩阵幂和向量的乘积?_**

将矩阵幂化为递推公式如下:

$$
\vec{x}_{k+1} = A\vec{x}_k (k=1, 2, 3, ...)
$$

```
对于该计算, 可以将初始向量的特征向量的线性组合 x = aU + bV, 进而求出通项公式
```

**_如何判断一个矩阵是否为正定矩阵?_**

$$
如果一个矩阵A正定, 那么对应的二次型\vec{x}^T A \vec{x} > 0 (\vec{x}不为零向量)恒成立
$$

```
显然如果特征值全部>0, 则二次型>0, 则正定
```

## 6. 矩阵分解
